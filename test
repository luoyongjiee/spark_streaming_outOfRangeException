import kafka.common.TopicAndPartition
import kafka.message.MessageAndMetadata
import kafka.serializer.Decoder
import org.apache.spark.SparkException
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka.{HasOffsetRanges, KafkaCluster, KafkaUtils}
import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset

import scala.collection.mutable.ArrayBuffer
import scala.reflect.ClassTag
import scala.tools.nsc.interpreter.InputStream

/**
  * Created by 17 on 2019/1/28.
  */
class KafkaManger (val kafkaParams: Map[String,String]) extends Serializable{

  private val kc = new KafkaCluster(kafkaParams)

  /**
    * 创建数据流前，根据实际情况更新消费offset
    * @param topics
    * @param groupId
    */
  def setOrUpdateOffsets(topics: Set[String], groupId: String): Unit = {
    topics.foreach(topic => {
      var hasConsumed = true
      val partitionsE = kc.getPartitions(Set(topic))
      if (partitionsE.isLeft)
        throw new SparkException(s"get kafka partition failed:${partitionsE.left.get}")
      val partitions = partitionsE.right.get
      val consumerOffsetsE = kc.getConsumerOffsets(groupId,partitions)
      if (consumerOffsetsE.isLeft) hasConsumed = false
      if (hasConsumed) {
        //消费过
        /**
          * 如果实时计算程序执行时出现kafka.common.OffsetOutOfRangeExpection,
          * 说明zk上保存的offsets已经过时了，即kafka的定时清理策略已经将包含该offset的文件删除
          * 针对这种情况，只要判断zk上的consumerOffset和earliestLeaderOffset的大小
          * 如果consumerOffset比earlistLeaderOffsets还小的话，说明consumer已经过时
          * 这时把ConsumerOffsets更新为earliesLearOffsets
          */
        val eraliestLearOffset = kc.getEarliestLeaderOffsets(partitions)
        if (eraliestLearOffset.isLeft)
          throw new SparkException(s"get earliestleard offset failed:${eraliestLearOffset.left.get}")
        val earkuestLeaderOffsets = eraliestLearOffset.right.get
        val consumerOffsets = consumerOffsetsE.right.get

        //如果存在部分分区consumerOffsets过时，只要更新该分区的offset为最新
        var offsets: Map[TopicAndPartition,Long] = Map()
        consumerOffsets.foreach({case (tp,n) =>
          val earliestLeaderOffset = earkuestLeaderOffsets(tp).offset
          if (n < earliestLeaderOffset) {
            println("consumer group:" + groupId + ",topic:" + tp.topic + ",partition:" + tp.partition +
              "offsets已经过时，更新为" + earliestLeaderOffset)
            offsets += (tp -> earliestLeaderOffset)
          }
        })
        if (offsets.nonEmpty)
          kc.setConsumerOffsets(groupId,offsets)
      } else {
        //没有消费过
        val reset = kafkaParams.get("auto.offset.reset").map(_.toLowerCase())
        var leaderOffsets: Map[TopicAndPartition, LeaderOffset] = null
        if (reset == Some("smallest")) {
          val leaderOffsetsE = kc.getEarliestLeaderOffsets(partitions)
          if (leaderOffsetsE.isLeft)
            throw new SparkException(s"get earliest leader offsets failed: ${leaderOffsetsE.left.get}")
          leaderOffsets = leaderOffsetsE.right.get
        } else {
          val leaderOffsetsE = kc.getEarliestLeaderOffsets(partitions)
          if (leaderOffsetsE.isLeft)
            throw new SparkException(s"get latest leader offsets failed: ${leaderOffsetsE.left.get}")
          leaderOffsets = leaderOffsetsE.right.get
        }
        val offsets = leaderOffsets.map {
          case (tp, offset) => (tp, offset.offset)
        }
        kc.setConsumerOffsets(groupId, offsets)
      }
    })
  }

  /**
    * 创建数据流
    */
  def createDstream[K: ClassTag, V: ClassTag, KD <: Decoder[K] : ClassTag, VD <: Decoder[V] : ClassTag](
                                                                                                         ssc: StreamingContext,
                                                                                                         kafkaParams: Map[String,String],
                                                                                                         topics: Set[String]): InputDStream[(K,V)] = {

    val groupId = kafkaParams.get("group.id").get
    setOrUpdateOffsets(topics,groupId)

    //从zookeeper上读取offset开始消费msg
    val messages = {
      val partitionsE = kc.getPartitions(topics)
      if (partitionsE.isLeft)
        throw new SparkException(s"get kafka partition failed: ${partitionsE.left.get}")
      val partitions = partitionsE.right.get

      println("partitions",partitions)

      val consumerOffsetsE = kc.getConsumerOffsets(groupId,partitions)
      if (consumerOffsetsE.isLeft)
        throw new SparkException(s"get kafka consumer offsets failed: ${consumerOffsetsE.left.get}")
      val consumerOffsets = consumerOffsetsE.right.get
      KafkaUtils.createDirectStream(
        ssc,kafkaParams,consumerOffsets,(mmd:MessageAndMetadata[K,V]) => (mmd.key, mmd.message))
    }
    messages
  }

  /**
    *
    * @param ssc
    * @param kafkaParams
    * @param topicPartition
    * @tparam K
    * @tparam V
    * @tparam KD
    * @tparam VD
    * @return
    */
  def createDirectStreamByAssignPartition[K: ClassTag, V: ClassTag, KD <: Decoder[K] : ClassTag, VD <: Decoder[V] : ClassTag](
                                                                                                                               ssc: StreamingContext,
                                                                                                                               kafkaParams: Map[String,String],
                                                                                                                               topicPartition: Set[TopicAndPartition]): InputDStream[(K,V)] = {
    val groupId = kafkaParams.get("group.id").get
    //在zookeeper上读取offsets前先根据实际情况更新offsets
    var topics = ArrayBuffer[String]()
    val tpArray = topicPartition.toArray
    for (i <- 0 until tpArray.length) {
      topics += tpArray(i).topic
    }

    setOrUpdateOffsets(topics.toSet,groupId)
    //从zookeeper上读取offset开始消费msg
    val messages = {
      val consumerOffsetE = kc.getConsumerOffsets(groupId,topicPartition)
      if (consumerOffsetE.isLeft)
        throw new SparkException(s"get kafka consumer offsets failed: ${consumerOffsetE.left.get}")
      val consumerOffsets = consumerOffsetE.right.get
      KafkaUtils.createDirectStream(ssc,kafkaParams,consumerOffsets,(mmd:MessageAndMetadata[K,V]) => (mmd.key(),mmd.message()))
    }
    messages
  }

  def updateZKOffsets(rdd: RDD[(String,String)]): Unit = {
    val groupId = kafkaParams.get("group.id").get
    val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges

    for (offset <- offsetsList) {
      val topicAndPartition = TopicAndPartition(offset.topic,offset.partition)
      val o = kc.setConsumerOffsets(groupId, Map((topicAndPartition,offset.untilOffset)))
      if (o.isLeft) {
        println(s"Error updating the offset to kafka cluster:${o.left.get}")
      }
    }
  }

}
